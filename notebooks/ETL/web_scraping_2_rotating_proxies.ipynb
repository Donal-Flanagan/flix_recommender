{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34baaeb1-c772-4c29-9358-b6b1022d407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424666f-875c-4267-85ed-0e0f74f1ba8a",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72f26c-a773-48f0-8142-2b04d2e06d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_csv(\n",
    "    '../data/ml-latest-small/links.csv', \n",
    "    usecols=['movieId', 'imdbId', 'tmdbId'],\n",
    "    dtype={'movieId': int, 'imdbId': str, 'tmdbId': str}\n",
    ")\n",
    "links_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d09a40-108d-4ff8-9fdf-042f6145469b",
   "metadata": {},
   "source": [
    "# Scrape metadata \n",
    "## Implement rotating proxies\n",
    "We will choosing an IP address at random from a list of free proxies sourced from https://free-proxy-list.net/ and https://proxyspace.pro/\n",
    "\n",
    "We will also add a random fake user-agent header to the requests to make it look like the request is coming from a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9defa2e-f01c-4148-be70-3237cc00fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n",
    "]\n",
    "\n",
    "class TmdbWebScraper:\n",
    "    BASE_URL = \"https://www.themoviedb.org/movie/\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 proxy_list=None,\n",
    "                 movie_dir_path='../data/ml-latest-small/',\n",
    "                 movie_people_filename='movie_cast_and_crew.csv',\n",
    "                 poster_links_filename='poster_links.csv',\n",
    "                 max_retries=3):\n",
    "\n",
    "        self.proxy_list = proxy_list.copy() if proxy_list else []\n",
    "        self.movie_dir_path = movie_dir_path\n",
    "        self.movie_people_filename = movie_people_filename\n",
    "        self.poster_links_filename = poster_links_filename\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "        self._movie_people = []\n",
    "        self._poster_links = []\n",
    "        self._failed_requests = []\n",
    "        self._crew_not_found = []\n",
    "        self._cast_not_found = []\n",
    "        self._poster_link_not_found = []\n",
    "        self._current_iteration = 0\n",
    "        self._movieIds_already_checked = []\n",
    "        self.last_reported_percent = -1\n",
    "        self.delay_min = 0.25\n",
    "        self.delay_max = 0.75\n",
    "        self.num_links_to_scrape = 0\n",
    "        self._scraping_stopped = False\n",
    "\n",
    "\n",
    "    def update_proxy_list(self, new_proxy_list):\n",
    "        \"\"\"Updates the proxy list only if it is a non-empty list.\"\"\"\n",
    "    \n",
    "        if not isinstance(new_proxy_list, list):\n",
    "            print(\"Error: Provided proxy list is not a list.\")\n",
    "            return \n",
    "    \n",
    "        if not new_proxy_list:\n",
    "            print(\"Error: Proxy list is empty. Please provide a list of proxies.\")\n",
    "            return \n",
    "    \n",
    "        self.proxy_list = new_proxy_list.copy()\n",
    "        print(f\"Proxy list updated successfully with {len(new_proxy_list)} proxies.\")\n",
    "\n",
    "        if self._scraping_stopped:\n",
    "            print(\"You can now rerun `TmdbWebScraper.aggregate_movie_casts_and_crews()` to resume scraping.\")\n",
    "            self._scraping_stopped = False\n",
    "        \n",
    "    def _get_random_proxy(self):\n",
    "        if not self.proxy_list: \n",
    "            self._scraping_stopped = True\n",
    "            return None\n",
    "            \n",
    "        return random.choice(self.proxy_list)\n",
    "\n",
    "    def _fetch_movie_data(self, movie_id, tmdb_id, session):\n",
    "        \"\"\" \n",
    "        Apparently TMDB has a rate limit of ~40 requests per 10 seconds so we want\n",
    "        to introduce a random time delay to make our scraper appear more human-like.\n",
    "        We will start at 200ms and work from there.\n",
    "        \"\"\"\n",
    "        if self._scraping_stopped:\n",
    "            print(f\"Skipping Movie {movie_id} due to lack of proxies.\")\n",
    "            self._failed_requests.append((movie_id, tmdb_id, \"No Proxy Available\"))\n",
    "            return None  \n",
    "        \n",
    "        url = f\"{self.BASE_URL}{tmdb_id}/\"\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            time.sleep(random.uniform(self.delay_min, self.delay_max))\n",
    "\n",
    "            proxy = self._get_random_proxy()\n",
    "            \n",
    "            if proxy is None:\n",
    "                print(f\"Skipping Movie {movie_id} due to lack of proxies.\")\n",
    "                self._failed_requests.append((movie_id, tmdb_id, \"No Proxy Available\"))\n",
    "                return None\n",
    "            \n",
    "            proxies = {\"https\": proxy}\n",
    "\n",
    "            try:\n",
    "                response = session.get(url, proxies=proxies, timeout=5) \n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    if \"captcha\" in response.text.lower() or len(response.text) < 500:  \n",
    "                        # TMDB might block with a bot detector or short HTML\n",
    "                        print(f\"Soft Block Detected for Movie {movie_id} - Response Length: {len(response.text)}\")\n",
    "                        self._failed_requests.append((movie_id, tmdb_id, \"Soft Block\"))\n",
    "                        return None\n",
    "                    return response.text\n",
    "    \n",
    "                elif response.status_code == 429:\n",
    "                    # If we still hit the rate limit then we use an exponential backoff up to 10 sec\n",
    "                    # and then increase our random delay range\n",
    "                    wait_time = min(2 ** attempt, 10)\n",
    "                    print(f\"Rate limit hit for Movie {movie_id}. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    self.delay_max *= 1.2\n",
    "                else:\n",
    "                    if response:\n",
    "                        print(f\"Request failed: MovieId {movie_id} - Status Code {response.status_code}\")\n",
    "                        self._failed_requests.append((movie_id, tmdb_id, response.status_code))\n",
    "                    else:\n",
    "                        self._failed_requests.append((movie_id, tmdb_id, \"No Response\"))\n",
    "\n",
    "            except requests.exceptions.RequestException:\n",
    "                print(f\"Proxy failed: {proxy}, switching to a new proxy...\")\n",
    "                try:\n",
    "                    self.proxy_list.remove(proxy)\n",
    "                    print(f\"Removed proxy: {proxy}\")\n",
    "                except ValueError:\n",
    "                    print(f\"Proxy {proxy} was not found in the list. It might have already been removed.\")\n",
    "                continue \n",
    "                \n",
    "        \n",
    "        return None \n",
    "        \n",
    "    def _scrape_movie_metadata_from_tmdb(self, movie_row, session):\n",
    "        movie_id = movie_row.movieId\n",
    "        tmdb_id = movie_row.tmdbId\n",
    "        people = []\n",
    "        poster_link = None\n",
    "\n",
    "        if movie_id not in self._movieIds_already_checked:\n",
    "        \n",
    "            response = self._fetch_movie_data(movie_id, tmdb_id, session)   \n",
    "    \n",
    "            if response:\n",
    "                soup = BeautifulSoup(response, 'html.parser')\n",
    "            \n",
    "                crew_list = soup.find('ol', class_='people no_image')\n",
    "                if crew_list:\n",
    "                    crew = crew_list.find_all('li', class_='profile')\n",
    "                    for person in crew:\n",
    "                        name = person.find('a').text\n",
    "                        jobs = person.find('p', class_='character').text\n",
    "                        jobs = [job.strip() for job in jobs.split(',')]\n",
    "                        for job in jobs:\n",
    "                            people.append((movie_id, name, job))\n",
    "                else:\n",
    "                    print(f\"Crew not found in html: MovieId {movie_id}\")\n",
    "                    self._crew_not_found.append((movie_id, tmdb_id))\n",
    "            \n",
    "                cast_list = soup.find('ol', class_='people scroller')\n",
    "                if cast_list:\n",
    "                    cast = cast_list.find_all('li', class_='card')\n",
    "                    for actor in cast:\n",
    "                        name_tag = actor.find('img', alt=True)\n",
    "                        actor_name = name_tag['alt'] if name_tag else \"Unknown\"\n",
    "                        people.append((movie_id, actor_name, 'Actor'))\n",
    "                else:\n",
    "                    print(f\"Cast not found in html: MovieId {movie_id}\")\n",
    "                    self._cast_not_found.append((movie_id, tmdb_id))\n",
    "                \n",
    "                poster_img_tag = soup.find('img', class_='poster w-full')\n",
    "                if poster_img_tag:\n",
    "                    poster_link = (movie_id, poster_img_tag.get('src'))\n",
    "                else:\n",
    "                    print(f\"Poster link not found in html: MovieId {movie_id}\")\n",
    "                    self._poster_link_not_found.append((movie_id, tmdb_id))\n",
    "\n",
    "                self._movieIds_already_checked.append(movie_id)\n",
    "    \n",
    "        return people, poster_link\n",
    "\n",
    "    def _track_percentage_completed(self):\n",
    "        percent_completed = round(self._current_iteration/self.num_links_to_scrape*100)\n",
    "        if percent_completed % 2 == 0 and percent_completed > self.last_reported_percent:\n",
    "            print(f\"{percent_completed}% completed\")\n",
    "            self.last_reported_percent = percent_completed\n",
    "\n",
    "    def aggregate_movie_casts_and_crews(self, links_df):\n",
    "    \n",
    "        print(\"Beginning movie metadata data aggregation.\")\n",
    "        \n",
    "        self.num_links_to_scrape = links_df.shape[0]\n",
    "\n",
    "        with requests.Session() as session:\n",
    "            # Add a random user-agent header to make it look like the request is  \n",
    "            # coming from a browser to stop the request being blocked.\n",
    "            session.headers.update({\"User-Agent\": random.choice(USER_AGENTS) })\n",
    "\n",
    "            for index, movie_row in links_df.iterrows():\n",
    "                if self._scraping_stopped:\n",
    "                    print(\"Scraping stopped due to lack of proxies. \")\n",
    "                    print(\"Please update the proxy list with the `TmdbWebScraper.update_proxy_list(proxy_list:List)` method.\")\n",
    "                    print(\"Then rerun `TmdbWebScraper.aggregate_movie_casts_and_crews()` to continue scraping.\")\n",
    "                    print(\"Exiting...\")\n",
    "                    break \n",
    "                movie_cast_and_crew, poster_link = self._scrape_movie_metadata_from_tmdb(movie_row, session)\n",
    "\n",
    "                if movie_cast_and_crew:\n",
    "                    self._movie_people.extend(movie_cast_and_crew)\n",
    "                if poster_link:\n",
    "                    self._poster_links.append(poster_link)\n",
    "\n",
    "                self._current_iteration += 1\n",
    "                self._track_percentage_completed()\n",
    "                \n",
    "        print(\"Data aggregation complete.\")\n",
    "\n",
    "    def rerun_aggregation_for_failed_requests(self):\n",
    "        # Reset the counters\n",
    "        self._current_iteration = 0\n",
    "        self.last_reported_percent = -1\n",
    "        \n",
    "        failed_requests_df = self.get_failed_requests_df()\n",
    "        \n",
    "        # Reset the list for tracking failed requests\n",
    "        self._failed_requests = []\n",
    "        self.aggregate_movie_casts_and_crews(failed_requests_df)\n",
    "        \n",
    "    def get_movie_cast_and_crew_df(self):\n",
    "        return pd.DataFrame(self._movie_people, columns=['movieId', 'name', 'role'])\n",
    "\n",
    "    def get_poster_links_df(self):\n",
    "        return pd.DataFrame(self._poster_links, columns=['movieId', 'poster_link'])\n",
    "\n",
    "    def get_failed_requests_df(self):\n",
    "        return pd.DataFrame(self._failed_requests, columns=['movieId', 'tmdbId', 'statusCode'])\n",
    "\n",
    "    def get_crew_not_found_df(self):\n",
    "        return pd.DataFrame(self._crew_not_found, columns=['movieId', 'tmdbId'])\n",
    "        \n",
    "    def get_cast_not_found_df(self):\n",
    "        return pd.DataFrame(self._cast_not_found, columns=['movieId', 'tmdbId'])\n",
    "\n",
    "    def get_poster_link_not_found_df(self):\n",
    "        return pd.DataFrame(self._poster_link_not_found, columns=['movieId', 'tmdbId'])\n",
    "        \n",
    "    def save_movie_cast_and_crew_data(self):\n",
    "        self.get_movie_cast_and_crew_df.to_csv(self.movie_dir_path+self.movie_people_filename, index=False)\n",
    "        print(f\"Movie cast & crew saved to {self.movie_dir_path}{self.movie_people_filename}\")\n",
    "\n",
    "    def save_poster_links_data(self):\n",
    "        self.get_poster_links_df.to_csv(self.movie_dir_path+self.poster_links_filename, index=False)\n",
    "        print(f\"Poster links saved to {self.movie_dir_path}{self.poster_links_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e26fc7-5bfe-4751-b3a5-4006d6bb58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_list = [\n",
    "    \"https://8.219.97.248\",\n",
    "    \"https://67.43.228.250\",\n",
    "    \"https://13.245.75.139\",\n",
    "    \"https://222.252.194.204\",\n",
    "    \"https://51.210.54.186\",\n",
    "    \"https://204.236.137.68\",\n",
    "    \"https://63.35.64.177\",\n",
    "    \"https://113.160.133.32\",\n",
    "    \"https://3.90.100.12\",\n",
    "    \"https://13.59.156.167\",\n",
    "    \"https://54.152.3.36\",\n",
    "    \"https://3.130.65.162\",\n",
    "    \"https://52.67.10.183\",\n",
    "    \"https://18.228.198.164\",\n",
    "    \"https://54.233.119.172\",\n",
    "    \"https://46.51.249.135\",\n",
    "    \"https://3.123.150.192\",\n",
    "    \"https://18.228.149.161\",\n",
    "    \"https://3.124.133.93\",\n",
    "    \"https://3.127.62.252\",\n",
    "    \"https://35.79.120.242\",\n",
    "    \"https://35.72.118.126\",\n",
    "    \"https://3.126.147.182\",\n",
    "    \"https://2.59.181.125\",\n",
    "    \"https://3.136.29.104\",\n",
    "    \"https://13.56.192.187\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15853555-1f2a-4918-8fe8-6ebb45152553",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_csv(\n",
    "    '../data/ml-latest-small/links.csv', \n",
    "    usecols=['movieId', 'imdbId', 'tmdbId'],\n",
    "    dtype={'movieId': int, 'imdbId': str, 'tmdbId': str}\n",
    ")\n",
    "movies_missing_tmdbIds = links_df[links_df.tmdbId.isna()]\n",
    "links_df = links_df[~links_df.movieId.isin(movies_missing_tmdbIds.movieId)]\n",
    "\n",
    "scraper = TmdbWebScraper(proxy_list)\n",
    "scraper.aggregate_movie_casts_and_crews(links_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f34efc-ecd6-41e9-945d-9a3b618b925c",
   "metadata": {},
   "source": [
    "## Result: Failure - The website is identifying the proxies immediately and blocking the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c7c3b-b276-463d-8bf0-72ba8650ae8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b3d78-3f5e-47b1-a9c4-f368e27f3b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40011c87-3e1e-4130-9635-ad54ba6e7c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ca7aa-e5fd-4c85-8714-a7e15b59a471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b59d42-8a07-4d59-8c46-c67c6f382356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
